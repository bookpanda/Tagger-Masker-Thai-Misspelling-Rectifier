{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12792,"status":"ok","timestamp":1662439978854,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"IwEI9jZ9yb8G","outputId":"78393243-2928-4d18-95b4-f2a9ed730a27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.15.0\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 8.4 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 60.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 72.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (4.64.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 70.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (2022.6.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 31.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.15.0) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.15.0) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (2022.6.15)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.15.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.15.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.15.0) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=6f74dc9f1bc4dfea55a1c493f0d80eef8e86cbee74c006e788bc58fc95e9596a\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.9.1 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.10.3 transformers-4.15.0\n"]}],"source":["# BERT\n","!pip install transformers==4.15.0 sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YbwI_6aWu49u","executionInfo":{"status":"ok","timestamp":1662439981894,"user_tz":-420,"elapsed":3047,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["import pandas as pd\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch \n","import numpy as np\n","from transformers import BertTokenizerFast, BertForTokenClassification, AutoTokenizer\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from torch.optim import SGD\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"rcSzaFGhvuna"},"source":["# Read CSV Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17814,"status":"ok","timestamp":1662440221654,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"_S6FWxqDicRZ","outputId":"cf92cd73-ff29-4019-ae4d-322e3802852f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":5982,"status":"ok","timestamp":1662440238090,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"f8cnvMNLu7CO","outputId":"4b6c4c61-7b7f-4ee4-e2e8-e25057da54bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    text  \\\n","15571  {'input_ids': [[tensor(5), tensor(10), tensor(...   \n","15572  {'input_ids': [[tensor(5), tensor(1896), tenso...   \n","15573  {'input_ids': [[tensor(5), tensor(2169), tenso...   \n","15574  {'input_ids': [[tensor(5), tensor(10), tensor(...   \n","15575  {'input_ids': [[tensor(5), tensor(5960), tenso...   \n","\n","                                                  labels  \n","15571  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n","15572  [tensor(0), tensor(0), tensor(1), tensor(1), t...  \n","15573  [tensor(0), tensor(0), tensor(1), tensor(0), t...  \n","15574  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n","15575  [tensor(0), tensor(0), tensor(0), tensor(0), t...  "],"text/html":["\n","  <div id=\"df-97ad4cf3-73d0-46fe-8ecd-d2e3d8b4fe23\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15571</th>\n","      <td>{'input_ids': [[tensor(5), tensor(10), tensor(...</td>\n","      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n","    </tr>\n","    <tr>\n","      <th>15572</th>\n","      <td>{'input_ids': [[tensor(5), tensor(1896), tenso...</td>\n","      <td>[tensor(0), tensor(0), tensor(1), tensor(1), t...</td>\n","    </tr>\n","    <tr>\n","      <th>15573</th>\n","      <td>{'input_ids': [[tensor(5), tensor(2169), tenso...</td>\n","      <td>[tensor(0), tensor(0), tensor(1), tensor(0), t...</td>\n","    </tr>\n","    <tr>\n","      <th>15574</th>\n","      <td>{'input_ids': [[tensor(5), tensor(10), tensor(...</td>\n","      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n","    </tr>\n","    <tr>\n","      <th>15575</th>\n","      <td>{'input_ids': [[tensor(5), tensor(5960), tenso...</td>\n","      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97ad4cf3-73d0-46fe-8ecd-d2e3d8b4fe23')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-97ad4cf3-73d0-46fe-8ecd-d2e3d8b4fe23 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-97ad4cf3-73d0-46fe-8ecd-d2e3d8b4fe23');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["df = pickle.load(open('drive/MyDrive/AIBuilders/json/ner_json_15k.pkl', 'rb'))\n","df.tail()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1662440238091,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"MzZEQxj94gwC","outputId":"d78118cf-71f3-47b5-f70f-dffcf7d950ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([   0,    0,    1,    1,    1,    0,    0,    0,    0,    0, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100])"]},"metadata":{},"execution_count":5}],"source":["df.iloc[0]['labels']"]},{"cell_type":"markdown","metadata":{"id":"mosLtuxwv2hm"},"source":["# Initialize Tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VdHlzKiIvyI-","executionInfo":{"status":"ok","timestamp":1662440286555,"user_tz":-420,"elapsed":1049,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["# tokenizer_th = AutoTokenizer.from_pretrained('airesearch/wangchanberta-base-att-spm-uncased', model_max_length=512)\n","tokenizer_th = pickle.load(open('drive/MyDrive/AIBuilders/json/tokenizer_json_15k.pkl', 'rb'))"]},{"cell_type":"markdown","metadata":{"id":"ZwRa6UmDv6g4"},"source":["# Create Dataset Class"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uz8ru0XEv0yd","executionInfo":{"status":"ok","timestamp":1662440242678,"user_tz":-420,"elapsed":362,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["class DataSequence(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","\n","        # lb = [i.split() for i in df['labels'].values.tolist()]\n","        # txt = df['text'].values.tolist()\n","        # self.texts = [tokenizer_th(str(i),\n","        #                        padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n","        # self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n","        self.texts = [df.iloc[i]['text'] for i in range(df.shape[0])]\n","        self.labels = [df.iloc[i]['labels'] for i in range(df.shape[0])]\n","\n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_batch_data(self, idx):\n","\n","        return self.texts[idx]\n","\n","    def get_batch_labels(self, idx):\n","\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","\n","        batch_data = self.get_batch_data(idx)\n","        batch_labels = self.get_batch_labels(idx)\n","\n","        return batch_data, batch_labels\n"]},{"cell_type":"markdown","metadata":{"id":"BY7BgOzRwBeQ"},"source":["# Split Data"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"HUdcW2Rbv-5d","executionInfo":{"status":"ok","timestamp":1662440245369,"user_tz":-420,"elapsed":516,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["df_test = df[10576:]\n","df = df[:10576]\n","df_train, df_val = np.split(df.sample(frac=1, random_state=42),\n","                            [int(.9 * len(df))])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1662440249823,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"sDVQGS1YVZtn","outputId":"4370d8b3-a574-4537-8c37-5d84d1c211aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'input_ids': tensor([[   5,   10, 1167, 1045,  878, 4240,   41, 3633,    6,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","              1,    1,    1,    1,    1,    1,    1,    1]]),\n","  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0]])},\n"," tensor([   0,    0,    0,    0,    0,    1,    1,    1,    0, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100]))"]},"metadata":{},"execution_count":8}],"source":["asdf = DataSequence(df_train)\n","asdf.__getitem__(1)"]},{"cell_type":"markdown","metadata":{"id":"N12N752nwGin"},"source":["# Build Model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gDSbpnWtwEmo","executionInfo":{"status":"ok","timestamp":1662440254224,"user_tz":-420,"elapsed":333,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["class BertModel(torch.nn.Module):\n","\n","    def __init__(self):\n","\n","        super(BertModel, self).__init__()\n","        self.bert = BertForTokenClassification.from_pretrained('airesearch/wangchanberta-base-att-spm-uncased', num_labels=2)\n","        self.bert.resize_token_embeddings(len(tokenizer_th))\n","\n","    def forward(self, input_id, mask, label):\n","\n","        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n","\n","        return output"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4444043a63004f54a0d652ba21b63951","1c4f22b19d1d4e1fa69b62662d42d0f5","904a68861aa24468a602086cbada6470","7f11220063af4bf0beba271190c265f4","47b711ce5f654e2fa9fc9fcfbdf7d830","73362dd7122348deb4d5946ba36682ea","b2f3f36bfcf54e168ed7f22f303441f8","c2cd00f62d534680b3bb5d49fb012d23","ece32913df5648ffa322335e752ddf08","5dc1500a44674b8e9f3717b92752e989","b9e0998084ad4b89af12e35a97623de3","f598d1248d8b4c6b9e8312ee3713939b","0fd1e9bc4fed4d80a3bb1ab990d47d02","cac1fbe6eb0745fd9b212021bbd8dfc4","18e77e4ce6d04ffbbb0900624cd07c3b","daaef1b62946476ea60537ff81a50722","fd39c8859fa349f3a4b75e0dd168dfa1","cbe67136d9bc4ee69b6c1966f40b7fe9","c57fc3d6a90a48d68227eebe1777b3b4","ec6f3cbcba2e4def9f68a3d2cf747f9d","cc65220fde9a4347921c99ae15c42201","e081e72541ad447e9d4c8f10ab6b0326"]},"executionInfo":{"elapsed":16312,"status":"ok","timestamp":1662440379061,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"Epie4o66Reup","outputId":"83d1f1d5-94db-4e1d-f37f-74f6ba149d15"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4444043a63004f54a0d652ba21b63951"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using a model of type camembert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/404M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f598d1248d8b4c6b9e8312ee3713939b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing BertForTokenClassification: ['roberta.encoder.layer.1.attention.self.query.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'lm_head.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['encoder.layer.7.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'classifier.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (bert): BertForTokenClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(25354, 768)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":11}],"source":["model = BertModel()\n","# model = BertForTokenClassification.from_pretrained('airesearch/wangchanberta-base-att-spm-uncased', num_labels=2)\n","# model.resize_token_embeddings(len(tokenizer_th))\n","\n","# FILE = \"drive/MyDrive/AIBuilders/tagging.pth\"\n","FILE = \"drive/MyDrive/AIBuilders/json/tagging_json_400.pth\"\n","model.load_state_dict(torch.load(FILE, map_location=torch.device('cpu')))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1655650258480,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"sqMWGt1MnH_w","outputId":"7bb2640f-705c-4d57-d681-d261b7569c04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login() # เอาไว้โยนโมเดลขึ้น hugging face ได้เลย"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1655651567919,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"vDm03Vdhopos","outputId":"c8a6b85a-17ab-4165-8961-005911abae32"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["from transformers import TrainingArguments\n","\n","model_checkpoint = \"airesearch/wangchanberta-base-att-spm-uncased\"\n","\n","batch_size = 32\n","# Show the training loss with every epoch\n","logging_steps = len(df_train) // batch_size\n","model_name = model_checkpoint.split(\"/\")[-1]\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_name}-tagging\",\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    push_to_hub=True,\n","    num_train_epochs = 75,\n","    fp16=True, # สำหรับคนใช้ GPU\n","    logging_steps=logging_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6136,"status":"ok","timestamp":1655651574576,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"LbTQONglp2Su","outputId":"dc5801ce-a374-448d-d1ce-f6ac853af042"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/wangchanberta-base-att-spm-uncased-tagging is already a clone of https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging. Make sure you pull the latest changes with `repo.git_pull()`.\n","Using amp half precision backend\n"]}],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=df_train,\n","    eval_dataset=df_test,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["00fd55d320cc43daa84d9128f243e217","4f657ca9ea6f40d596e438a8397a6164","37c30ad614fa4d7e92f0f56c8d83e03e","41ba46ad8168442ba5c97804e64c537c","8150574dae9a4e8e9f01c943f81cf48a","91f2805cdea443579516a4fd448bcec5","deb791c8e7bc4b49be62c6bf8272cf21","b862d06dab204371b1d19697fee092a9","755dc07041ef41808382ce856b28c114","5c54bd59f2cc44d2a2a31690e03fbda4","a1ad8789a4324143a7a442c111b18a2a","d627bbabb1bb4e10b6cf745d65075366","53cfe0ced88745219f05407caceea7db","89fc3fa575f14c78a6f915cba663e665","c62c380001fe400bbf780d3a322a08a5","a7f6d4c051f249748a18b68cad564765","a525cda209b64daba6d85f4f75fc0e50","cd27b78f3b694baeaab64120681bedf3","8585b55f4bb44dce8cc5fba495d4c48d","5c9139ad997e4421bce56c88197e3dc5","fcb2a9558f4c457fb0402fec760dd7f3","fbe141af7c004fef9d04f410eb6901b7"]},"executionInfo":{"elapsed":417581,"status":"ok","timestamp":1655651996877,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"nlabWhS4n_Gf","outputId":"f8b869f4-8d49-4063-f6bb-2a2d2df40cd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to wangchanberta-base-att-spm-uncased-tagging\n","Configuration saved in wangchanberta-base-att-spm-uncased-tagging/config.json\n","Model weights saved in wangchanberta-base-att-spm-uncased-tagging/pytorch_model.bin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00fd55d320cc43daa84d9128f243e217","version_major":2,"version_minor":0},"text/plain":["Upload file pytorch_model.bin:   0%|          | 3.34k/425M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d627bbabb1bb4e10b6cf745d65075366","version_major":2,"version_minor":0},"text/plain":["Upload file training_args.bin: 100%|##########| 2.92k/2.92k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["To https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging\n","   3c3ed19..daa973a  main -> main\n","\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Token Classification', 'type': 'token-classification'}}\n","To https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging\n","   daa973a..712de4d  main -> main\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging/commit/daa973a5e4f9d4068f6ffab57f3a3af8d9f1b64a'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1656648121019,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"emyGME48RA4a","outputId":"7d43f982-6d2e-451f-92fa-655325ac95d8"},"outputs":[{"data":{"text/plain":["BertModel(\n","  (bert): BertForTokenClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(25005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"C6zDk8yQwLqq"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0jSoWhNwJpN"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","def train_loop(model, df_train, df_val):\n","\n","    train_dataset = DataSequence(df_train)\n","    val_dataset = DataSequence(df_val)\n","\n","    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    best_acc = 0\n","    best_loss = 1000\n","    \n","    for epoch_num in range(EPOCHS):\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        model.train()\n","        \n","        for train_data, train_label in tqdm(train_dataloader):\n","            train_label = train_label[0].to(device)\n","            mask = train_data['attention_mask'][0].to(device)\n","            input_id = train_data['input_ids'][0].to(device)\n","            optimizer.zero_grad()\n","            loss, logits = model(input_id, mask, train_label)\n","\n","            logits_clean = logits[0][train_label != -100]\n","            label_clean = train_label[train_label != -100]\n","            predictions = logits_clean.argmax(dim=1)\n","            pred_len = len(predictions)\n","            for i in range(pred_len):\n","                if predictions[i] == 0:\n","                    predictions[i] = -100\n","            \n","            # print(predictions)\n","            # print(label_clean)\n","            numer = 0\n","            denom = 0\n","            for i in range(pred_len):\n","                if label_clean[i] == 1:\n","                    denom += 1\n","                    if predictions[i] == 1:\n","                        numer += 1\n","                elif label_clean[i] == 0 and predictions[i] == 1:\n","                    denom += 1\n","\n","            if denom == 0:\n","              acc = 0\n","            else:\n","              acc = float(numer)/float(denom)\n","            # print(f\"train acc: {acc}\")\n","            total_acc_train += acc * BATCH_SIZE\n","            total_loss_train += loss.item() * BATCH_SIZE\n","\n","            loss.backward()\n","            optimizer.step()\n","        \n","        model.eval()\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        for val_data, val_label in val_dataloader:\n","\n","            val_label = val_label[0].to(device)\n","            mask = val_data['attention_mask'][0].to(device)\n","\n","            input_id = val_data['input_ids'][0].to(device)\n","\n","            loss, logits = model(input_id, mask, val_label)\n","\n","            logits_clean = logits[0][val_label != -100]\n","            label_clean = val_label[val_label != -100]\n","            predictions = logits_clean.argmax(dim=1)   \n","            pred_len = len(predictions)\n","            for i in range(pred_len):\n","                if predictions[i] == 0:\n","                    predictions[i] = -100       \n","\n","            numer = 0\n","            denom = 0\n","            for i in range(pred_len):\n","                if label_clean[i] == 1:\n","                    denom += 1\n","                    if predictions[i] == 1:\n","                        numer += 1\n","                elif label_clean[i] == 0 and predictions[i] == 1:\n","                    denom += 1\n","        \n","            if denom == 0:\n","              acc = 0\n","            else:\n","              acc = float(numer)/float(denom)\n","            # print(f\"valid acc: {acc}\")\n","            total_acc_val += acc * BATCH_SIZE\n","            total_loss_val += loss.item() * BATCH_SIZE\n","\n","        val_accuracy = total_acc_val / len(df_val)\n","        val_loss = total_loss_val / len(df_val)\n","\n","        print(\n","            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n","\n","LEARNING_RATE = 1e-3\n","EPOCHS = 100\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzqsWQ0xOXU1","executionInfo":{"status":"ok","timestamp":1657011383939,"user_tz":-420,"elapsed":3407694,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}},"outputId":"8ee58eeb-397d-4744-e0f8-20b371e076b3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 1 | Loss:  0.087 | Accuracy:  0.880 | Val_Loss:  0.099 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 2 | Loss:  0.072 | Accuracy:  0.884 | Val_Loss:  0.097 | Accuracy:  0.872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 3 | Loss:  0.084 | Accuracy:  0.871 | Val_Loss:  0.086 | Accuracy:  0.886\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 4 | Loss:  0.073 | Accuracy:  0.873 | Val_Loss:  0.087 | Accuracy:  0.886\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 5 | Loss:  0.082 | Accuracy:  0.865 | Val_Loss:  0.090 | Accuracy:  0.886\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 6 | Loss:  0.070 | Accuracy:  0.902 | Val_Loss:  0.108 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 7 | Loss:  0.075 | Accuracy:  0.890 | Val_Loss:  0.095 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 8 | Loss:  0.082 | Accuracy:  0.866 | Val_Loss:  0.100 | Accuracy:  0.856\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 9 | Loss:  0.087 | Accuracy:  0.857 | Val_Loss:  0.096 | Accuracy:  0.856\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 10 | Loss:  0.089 | Accuracy:  0.855 | Val_Loss:  0.091 | Accuracy:  0.907\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 11 | Loss:  0.076 | Accuracy:  0.868 | Val_Loss:  0.088 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 12 | Loss:  0.085 | Accuracy:  0.873 | Val_Loss:  0.094 | Accuracy:  0.922\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 13 | Loss:  0.075 | Accuracy:  0.893 | Val_Loss:  0.084 | Accuracy:  0.894\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 14 | Loss:  0.084 | Accuracy:  0.871 | Val_Loss:  0.091 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 15 | Loss:  0.091 | Accuracy:  0.863 | Val_Loss:  0.085 | Accuracy:  0.871\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 16 | Loss:  0.067 | Accuracy:  0.883 | Val_Loss:  0.125 | Accuracy:  0.877\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 17 | Loss:  0.081 | Accuracy:  0.876 | Val_Loss:  0.098 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 18 | Loss:  0.092 | Accuracy:  0.860 | Val_Loss:  0.099 | Accuracy:  0.862\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 19 | Loss:  0.081 | Accuracy:  0.850 | Val_Loss:  0.091 | Accuracy:  0.902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 20 | Loss:  0.075 | Accuracy:  0.881 | Val_Loss:  0.095 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 21 | Loss:  0.082 | Accuracy:  0.867 | Val_Loss:  0.102 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 22 | Loss:  0.071 | Accuracy:  0.870 | Val_Loss:  0.083 | Accuracy:  0.869\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 23 | Loss:  0.075 | Accuracy:  0.875 | Val_Loss:  0.094 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 24 | Loss:  0.078 | Accuracy:  0.876 | Val_Loss:  0.103 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 25 | Loss:  0.072 | Accuracy:  0.872 | Val_Loss:  0.111 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 26 | Loss:  0.082 | Accuracy:  0.869 | Val_Loss:  0.093 | Accuracy:  0.877\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 27 | Loss:  0.076 | Accuracy:  0.881 | Val_Loss:  0.129 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 28 | Loss:  0.070 | Accuracy:  0.888 | Val_Loss:  0.118 | Accuracy:  0.877\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 29 | Loss:  0.074 | Accuracy:  0.874 | Val_Loss:  0.115 | Accuracy:  0.912\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 30 | Loss:  0.079 | Accuracy:  0.871 | Val_Loss:  0.106 | Accuracy:  0.902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 31 | Loss:  0.080 | Accuracy:  0.881 | Val_Loss:  0.095 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 32 | Loss:  0.068 | Accuracy:  0.893 | Val_Loss:  0.105 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 33 | Loss:  0.079 | Accuracy:  0.871 | Val_Loss:  0.099 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 34 | Loss:  0.075 | Accuracy:  0.877 | Val_Loss:  0.128 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 35 | Loss:  0.077 | Accuracy:  0.873 | Val_Loss:  0.081 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 36 | Loss:  0.085 | Accuracy:  0.860 | Val_Loss:  0.077 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 37 | Loss:  0.062 | Accuracy:  0.897 | Val_Loss:  0.108 | Accuracy:  0.842\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 38 | Loss:  0.069 | Accuracy:  0.885 | Val_Loss:  0.103 | Accuracy:  0.852\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 39 | Loss:  0.083 | Accuracy:  0.871 | Val_Loss:  0.107 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 40 | Loss:  0.081 | Accuracy:  0.875 | Val_Loss:  0.121 | Accuracy:  0.902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 41 | Loss:  0.072 | Accuracy:  0.881 | Val_Loss:  0.097 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 42 | Loss:  0.083 | Accuracy:  0.871 | Val_Loss:  0.088 | Accuracy:  0.877\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 43 | Loss:  0.064 | Accuracy:  0.894 | Val_Loss:  0.094 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 44 | Loss:  0.070 | Accuracy:  0.890 | Val_Loss:  0.115 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 45 | Loss:  0.059 | Accuracy:  0.911 | Val_Loss:  0.113 | Accuracy:  0.907\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 46 | Loss:  0.065 | Accuracy:  0.887 | Val_Loss:  0.091 | Accuracy:  0.907\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 47 | Loss:  0.076 | Accuracy:  0.880 | Val_Loss:  0.092 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 48 | Loss:  0.088 | Accuracy:  0.864 | Val_Loss:  0.085 | Accuracy:  0.912\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 49 | Loss:  0.074 | Accuracy:  0.884 | Val_Loss:  0.080 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 50 | Loss:  0.062 | Accuracy:  0.893 | Val_Loss:  0.091 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 51 | Loss:  0.055 | Accuracy:  0.914 | Val_Loss:  0.082 | Accuracy:  0.890\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 52 | Loss:  0.088 | Accuracy:  0.852 | Val_Loss:  0.081 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 53 | Loss:  0.065 | Accuracy:  0.894 | Val_Loss:  0.095 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 54 | Loss:  0.079 | Accuracy:  0.874 | Val_Loss:  0.081 | Accuracy:  0.907\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 55 | Loss:  0.061 | Accuracy:  0.897 | Val_Loss:  0.092 | Accuracy:  0.889\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 56 | Loss:  0.062 | Accuracy:  0.896 | Val_Loss:  0.082 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 57 | Loss:  0.073 | Accuracy:  0.886 | Val_Loss:  0.095 | Accuracy:  0.922\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 58 | Loss:  0.081 | Accuracy:  0.877 | Val_Loss:  0.096 | Accuracy:  0.862\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 59 | Loss:  0.077 | Accuracy:  0.882 | Val_Loss:  0.107 | Accuracy:  0.922\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 60 | Loss:  0.066 | Accuracy:  0.892 | Val_Loss:  0.149 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 61 | Loss:  0.073 | Accuracy:  0.874 | Val_Loss:  0.113 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 62 | Loss:  0.066 | Accuracy:  0.889 | Val_Loss:  0.092 | Accuracy:  0.891\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 63 | Loss:  0.071 | Accuracy:  0.877 | Val_Loss:  0.111 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 64 | Loss:  0.079 | Accuracy:  0.872 | Val_Loss:  0.120 | Accuracy:  0.902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 65 | Loss:  0.064 | Accuracy:  0.896 | Val_Loss:  0.102 | Accuracy:  0.907\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 66 | Loss:  0.056 | Accuracy:  0.906 | Val_Loss:  0.106 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 67 | Loss:  0.070 | Accuracy:  0.894 | Val_Loss:  0.088 | Accuracy:  0.861\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 68 | Loss:  0.078 | Accuracy:  0.890 | Val_Loss:  0.084 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 69 | Loss:  0.064 | Accuracy:  0.891 | Val_Loss:  0.090 | Accuracy:  0.886\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 70 | Loss:  0.073 | Accuracy:  0.889 | Val_Loss:  0.105 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 71 | Loss:  0.068 | Accuracy:  0.890 | Val_Loss:  0.107 | Accuracy:  0.887\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 72 | Loss:  0.072 | Accuracy:  0.884 | Val_Loss:  0.093 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 73 | Loss:  0.061 | Accuracy:  0.904 | Val_Loss:  0.102 | Accuracy:  0.907\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 74 | Loss:  0.085 | Accuracy:  0.869 | Val_Loss:  0.092 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 75 | Loss:  0.080 | Accuracy:  0.855 | Val_Loss:  0.122 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 76 | Loss:  0.064 | Accuracy:  0.889 | Val_Loss:  0.120 | Accuracy:  0.902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 77 | Loss:  0.069 | Accuracy:  0.887 | Val_Loss:  0.119 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 78 | Loss:  0.071 | Accuracy:  0.887 | Val_Loss:  0.106 | Accuracy:  0.866\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 79 | Loss:  0.061 | Accuracy:  0.890 | Val_Loss:  0.100 | Accuracy:  0.886\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 80 | Loss:  0.064 | Accuracy:  0.884 | Val_Loss:  0.112 | Accuracy:  0.872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 81 | Loss:  0.077 | Accuracy:  0.880 | Val_Loss:  0.121 | Accuracy:  0.902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 82 | Loss:  0.065 | Accuracy:  0.898 | Val_Loss:  0.090 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 83 | Loss:  0.060 | Accuracy:  0.903 | Val_Loss:  0.112 | Accuracy:  0.897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 84 | Loss:  0.074 | Accuracy:  0.875 | Val_Loss:  0.101 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 85 | Loss:  0.074 | Accuracy:  0.887 | Val_Loss:  0.114 | Accuracy:  0.877\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 86 | Loss:  0.066 | Accuracy:  0.911 | Val_Loss:  0.101 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 87 | Loss:  0.057 | Accuracy:  0.911 | Val_Loss:  0.099 | Accuracy:  0.882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 88 | Loss:  0.066 | Accuracy:  0.910 | Val_Loss:  0.085 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 89 | Loss:  0.056 | Accuracy:  0.899 | Val_Loss:  0.086 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 90 | Loss:  0.063 | Accuracy:  0.912 | Val_Loss:  0.120 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 91 | Loss:  0.067 | Accuracy:  0.886 | Val_Loss:  0.092 | Accuracy:  0.867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 92 | Loss:  0.072 | Accuracy:  0.881 | Val_Loss:  0.095 | Accuracy:  0.872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 93 | Loss:  0.073 | Accuracy:  0.890 | Val_Loss:  0.087 | Accuracy:  0.875\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 94 | Loss:  0.069 | Accuracy:  0.905 | Val_Loss:  0.089 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 95 | Loss:  0.072 | Accuracy:  0.896 | Val_Loss:  0.113 | Accuracy:  0.912\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 96 | Loss:  0.067 | Accuracy:  0.898 | Val_Loss:  0.089 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 97 | Loss:  0.068 | Accuracy:  0.895 | Val_Loss:  0.090 | Accuracy:  0.895\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 98 | Loss:  0.078 | Accuracy:  0.874 | Val_Loss:  0.098 | Accuracy:  0.872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 99 | Loss:  0.072 | Accuracy:  0.898 | Val_Loss:  0.096 | Accuracy:  0.892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 298/298 [00:32<00:00,  9.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 100 | Loss:  0.082 | Accuracy:  0.881 | Val_Loss:  0.089 | Accuracy:  0.882\n"]}],"source":["train_loop(model, df_train, df_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvzoFdbk8duB"},"outputs":[],"source":["# FILE = \"drive/MyDrive/AIBuilders/tagging_json_200.pth\"\n","FILE = \"tagging_json_400.pth\"\n","torch.save(model.state_dict(), FILE)"]},{"cell_type":"markdown","metadata":{"id":"4wQtjsdlxrQH"},"source":["# Evaluate Model"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224765,"status":"ok","timestamp":1662442410136,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"hYGHTi-cwPtA","outputId":"af46b9cd-0650-4a9f-d97c-fb2bedca379b"},"outputs":[{"output_type":"stream","name":"stdout","text":["=============100=================\n","=============200=================\n","=============300=================\n","=============400=================\n","=============500=================\n","=============600=================\n","=============700=================\n","=============800=================\n","=============900=================\n","=============1000=================\n","=============1100=================\n","=============1200=================\n","=============1300=================\n","=============1400=================\n","=============1500=================\n","=============1600=================\n","=============1700=================\n","=============1800=================\n","=============1900=================\n","=============2000=================\n","=============2100=================\n","=============2200=================\n","=============2300=================\n","=============2400=================\n","=============2500=================\n","=============2600=================\n","=============2700=================\n","=============2800=================\n","=============2900=================\n","=============3000=================\n","=============3100=================\n","=============3200=================\n","=============3300=================\n","=============3400=================\n","=============3500=================\n","=============3600=================\n","=============3700=================\n","=============3800=================\n","=============3900=================\n","=============4000=================\n","=============4100=================\n","=============4200=================\n","=============4300=================\n","=============4400=================\n","=============4500=================\n","=============4600=================\n","=============4700=================\n","=============4800=================\n","=============4900=================\n","=============5000=================\n","Test Accuracy:  0.878\n","0 >>> precision:  0.982    recall:  0.981    f1:  0.980\n","1 >>> precision:  0.920    recall:  0.937    f1:  0.914\n"]}],"source":["def evaluate(model, df_test):\n","\n","    test_dataset = DataSequence(df_test)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    total_acc_test = 0.0\n","\n","    cou = 0\n","    total_precision = [0,0]\n","    total_recall = [0,0]\n","    total_f1 = [0,0]\n","    for test_data, test_label in test_dataloader:\n","        # if cou == 20:\n","        #     break\n","        cou+=1\n","        if cou%100==0:\n","          print(f\"============={cou}=================\")\n","\n","        test_label = test_label[0].to(device)\n","        mask = test_data['attention_mask'][0].to(device)\n","        input_id = test_data['input_ids'][0].to(device)\n","          \n","        loss, logits = model(input_id, mask, test_label.long())\n","        # print(test_data)\n","        # print(test_label)\n","        # print(loss)\n","        # print(logits)\n","\n","        logits_clean = logits[0][test_label != -100]\n","        label_clean = test_label[test_label != -100]\n","        predictions = logits_clean.argmax(dim=1)\n","        pred_len = len(predictions)\n","        for i in range(pred_len):\n","            if predictions[i] == 0:\n","                predictions[i] = -100\n","        # print(logits_clean)\n","        # print(label_clean)\n","        # print(predictions)\n","\n","  \n","        a = tokenizer_th.convert_ids_to_tokens(input_id.squeeze())\n","        # print(f\"input: {''.join(a)}\")\n","        # print(label_clean)\n","        # print(len(input_id[0]), len(label_clean))\n","        # print(\"________________________\")\n","\n","        ids_to_labels = {0:'f', 1:'i', -100:'f'}\n","        prediction_label = [ids_to_labels[i] for i in predictions.tolist()]\n","        # print(\"PREDICTIONS\")\n","        # for i in range(len(predictions)):\n","        #   if prediction_label[i] == 'i':\n","        #     print(a[i])\n","        # print(\"LABELS\")\n","        # for i in range(len(label_clean)):\n","        #   if label_clean[i] == 1:\n","        #     print(a[i])   \n","\n","        # acc = (predictions == label_clean).float().mean()\n","\n","        # print(predictions, label_clean)\n","        TP = [0,0]\n","        FP = [0,0]\n","        TN = [0,0]\n","        FN = [0,0]\n","        for i in range(pred_len):\n","          if predictions[i] == 1 and label_clean[i] == 1:\n","            TP[1] += 1\n","            TN[0] += 1\n","          elif predictions[i] == -100 and label_clean[i] == 1:\n","            FP[0] += 1\n","            FN[1] += 1\n","          elif predictions[i] == 1 and label_clean[i] == 0:\n","            FP[1] += 1\n","            FN[0] += 1\n","          elif predictions[i] == -100 and label_clean[i] == 0:\n","            TP[0] += 1\n","            TN[1] += 1\n","\n","        precision = [float(TP[0]) / float(TP[0]+FP[0]) if TP[0]+FP[0] > 0 else 0, float(TP[1]) / float(TP[1]+FP[1]) if TP[1]+FP[1] > 0 else 0]\n","        recall = [float(TP[0]) / float(TP[0]+FN[0]) if TP[0]+FN[0] > 0 else 0, float(TP[1]) / float(TP[1]+FN[1]) if TP[1]+FN[1] > 0 else 0]\n","        f1 = [float(2*precision[0]*recall[0]) / float(precision[0] + recall[0]) if precision[0]+recall[0] > 0 else 0, float(2*precision[1]*recall[1]) / float(precision[1] + recall[1]) if precision[1]+recall[1] > 0 else 0]\n","        # print(f\"0 >>> TP:{TP[0]}   TN:{TN[0]}   FP:{FP[0]}   FN:{FN[0]}   precision:{precision[0]}    recall:{recall[0]}    f1:{f1[0]}\")\n","        # print(f\"1 >>> TP:{TP[1]}   TN:{TN[1]}   FP:{FP[1]}   FN:{FN[1]}   precision:{precision[1]}    recall:{recall[1]}    f1:{f1[1]}\")\n","        total_precision[0] += precision[0]\n","        total_precision[1] += precision[1]\n","        total_recall[0] += recall[0]\n","        total_recall[1] += recall[1]\n","        total_f1[0] += f1[0]\n","        total_f1[1] += f1[1]\n","\n","        numer = 0\n","        denom = 0\n","        for i in range(pred_len):\n","            if label_clean[i] == 1:\n","                denom += 1\n","                if predictions[i] == 1:\n","                    numer += 1\n","            elif label_clean[i] == 0 and predictions[i] == 1:\n","                denom += 1\n","        \n","        acc = float(numer)/float(denom)\n","        # print(f\"ACC: {acc}\")\n","        # print(\"-----------------------------------\")\n","        total_acc_test += acc\n","\n","    val_accuracy = total_acc_test / len(df_test)\n","    # print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n","    print(f'Test Accuracy: {total_acc_test / cou: .3f}')\n","    print(f'0 >>> precision: {total_precision[0] / cou: .3f}    recall: {total_recall[0] / cou: .3f}    f1: {total_f1[0] / cou: .3f}')\n","    print(f'1 >>> precision: {total_precision[1] / cou: .3f}    recall: {total_recall[1] / cou: .3f}    f1: {total_f1[1] / cou: .3f}')\n","\n","\n","evaluate(model, df_test)"]},{"cell_type":"markdown","metadata":{"id":"EyK8C2_fxvk6"},"source":["# Predict One Sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1657012360433,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"T99Q2GKQxtzK","outputId":"b94e4692-3848-4d5a-bfc8-e9623947bcf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["ids_to_tokens: ['<s>', '▁', 'ประเทศ', 'เรา', 'ผลิต', 'และ', 'ส่งออก', 'ยาสูบ', 'เยอะ', 'สุดในโลก', 'จิง', 'ป่าว', 'คับ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","['f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'i', 'i', 'i']\n","['<s>', '▁', 'ประเทศ', 'เรา', 'ผลิต', 'และ', 'ส่งออก', 'ยาสูบ', 'เยอะ', 'สุดในโลก', 'จิง', 'ป่าว', 'คับ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","สุดในโลก จิง ป่าว\n","จิง ป่าว คับ\n","ป่าว คับ </s>\n"]}],"source":["def align_word_ids(texts):\n","  \n","    tokenized_inputs = tokenizer_th(texts, padding='max_length', max_length=512, truncation=True)\n","    c = tokenizer_th.convert_ids_to_tokens(tokenized_inputs.input_ids)\n","    print(f\"ids_to_tokens: {c}\")\n","\n","    word_ids = tokenized_inputs.word_ids()\n","    previous_word_idx = None\n","    label_ids = []\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        else:\n","            try:\n","              label_ids.append(2)\n","            except:\n","                label_ids.append(-100)\n","\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","\n","def evaluate_one_text(model, sentence):\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    text = tokenizer_th(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","\n","    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n","   \n","    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n","    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n","   \n","    logits = model(input_id, mask, None)\n","    # print(input_id)\n","    # print(mask)\n","    # print(label_ids)\n","    logits_clean = logits[0][label_ids != -100]\n","    # print(logits_clean)\n","    # print(len(logits[0][0]), len(logits_clean))\n","\n","    predictions = logits_clean.argmax(dim=1).tolist()\n","    ids_to_labels = {0:'f', 1:'i'}\n","    prediction_label = [ids_to_labels[i] for i in predictions]\n","    print(prediction_label)\n","\n","    tokenized_inputs = tokenizer_th(sentence, padding='max_length', max_length=512, truncation=True)\n","    c = tokenizer_th.convert_ids_to_tokens(tokenized_inputs.input_ids)\n","    print(c)\n","    for i, j in enumerate(prediction_label):\n","        if j == 'i':\n","            print(c[i], c[i+1], c[i+2])\n","            \n","evaluate_one_text(model, 'ประเทศเราผลิตและส่งออกยาสูบเยอะสุดในโลกจิงป่าวคับ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-o6XaM5xzLR"},"outputs":[],"source":["FILE = \"tagging.pth\"\n","torch.save(model.bert.state_dict(), FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhWcAJXUVNU-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00fd55d320cc43daa84d9128f243e217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f657ca9ea6f40d596e438a8397a6164","IPY_MODEL_37c30ad614fa4d7e92f0f56c8d83e03e","IPY_MODEL_41ba46ad8168442ba5c97804e64c537c"],"layout":"IPY_MODEL_8150574dae9a4e8e9f01c943f81cf48a"}},"37c30ad614fa4d7e92f0f56c8d83e03e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b862d06dab204371b1d19697fee092a9","max":445293041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_755dc07041ef41808382ce856b28c114","value":445293041}},"41ba46ad8168442ba5c97804e64c537c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c54bd59f2cc44d2a2a31690e03fbda4","placeholder":"​","style":"IPY_MODEL_a1ad8789a4324143a7a442c111b18a2a","value":" 425M/425M [06:36&lt;00:00, 697kB/s]"}},"4f657ca9ea6f40d596e438a8397a6164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91f2805cdea443579516a4fd448bcec5","placeholder":"​","style":"IPY_MODEL_deb791c8e7bc4b49be62c6bf8272cf21","value":"Upload file pytorch_model.bin: 100%"}},"53cfe0ced88745219f05407caceea7db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a525cda209b64daba6d85f4f75fc0e50","placeholder":"​","style":"IPY_MODEL_cd27b78f3b694baeaab64120681bedf3","value":"Upload file training_args.bin: 100%"}},"5c54bd59f2cc44d2a2a31690e03fbda4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c9139ad997e4421bce56c88197e3dc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"755dc07041ef41808382ce856b28c114":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8150574dae9a4e8e9f01c943f81cf48a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8585b55f4bb44dce8cc5fba495d4c48d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89fc3fa575f14c78a6f915cba663e665":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8585b55f4bb44dce8cc5fba495d4c48d","max":2991,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c9139ad997e4421bce56c88197e3dc5","value":2991}},"91f2805cdea443579516a4fd448bcec5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1ad8789a4324143a7a442c111b18a2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a525cda209b64daba6d85f4f75fc0e50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f6d4c051f249748a18b68cad564765":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b862d06dab204371b1d19697fee092a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62c380001fe400bbf780d3a322a08a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcb2a9558f4c457fb0402fec760dd7f3","placeholder":"​","style":"IPY_MODEL_fbe141af7c004fef9d04f410eb6901b7","value":" 2.92k/2.92k [06:35&lt;?, ?B/s]"}},"cd27b78f3b694baeaab64120681bedf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d627bbabb1bb4e10b6cf745d65075366":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53cfe0ced88745219f05407caceea7db","IPY_MODEL_89fc3fa575f14c78a6f915cba663e665","IPY_MODEL_c62c380001fe400bbf780d3a322a08a5"],"layout":"IPY_MODEL_a7f6d4c051f249748a18b68cad564765"}},"deb791c8e7bc4b49be62c6bf8272cf21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbe141af7c004fef9d04f410eb6901b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcb2a9558f4c457fb0402fec760dd7f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4444043a63004f54a0d652ba21b63951":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c4f22b19d1d4e1fa69b62662d42d0f5","IPY_MODEL_904a68861aa24468a602086cbada6470","IPY_MODEL_7f11220063af4bf0beba271190c265f4"],"layout":"IPY_MODEL_47b711ce5f654e2fa9fc9fcfbdf7d830"}},"1c4f22b19d1d4e1fa69b62662d42d0f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73362dd7122348deb4d5946ba36682ea","placeholder":"​","style":"IPY_MODEL_b2f3f36bfcf54e168ed7f22f303441f8","value":"Downloading: 100%"}},"904a68861aa24468a602086cbada6470":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2cd00f62d534680b3bb5d49fb012d23","max":546,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ece32913df5648ffa322335e752ddf08","value":546}},"7f11220063af4bf0beba271190c265f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dc1500a44674b8e9f3717b92752e989","placeholder":"​","style":"IPY_MODEL_b9e0998084ad4b89af12e35a97623de3","value":" 546/546 [00:00&lt;00:00, 16.9kB/s]"}},"47b711ce5f654e2fa9fc9fcfbdf7d830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73362dd7122348deb4d5946ba36682ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2f3f36bfcf54e168ed7f22f303441f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2cd00f62d534680b3bb5d49fb012d23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ece32913df5648ffa322335e752ddf08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5dc1500a44674b8e9f3717b92752e989":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9e0998084ad4b89af12e35a97623de3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f598d1248d8b4c6b9e8312ee3713939b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fd1e9bc4fed4d80a3bb1ab990d47d02","IPY_MODEL_cac1fbe6eb0745fd9b212021bbd8dfc4","IPY_MODEL_18e77e4ce6d04ffbbb0900624cd07c3b"],"layout":"IPY_MODEL_daaef1b62946476ea60537ff81a50722"}},"0fd1e9bc4fed4d80a3bb1ab990d47d02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd39c8859fa349f3a4b75e0dd168dfa1","placeholder":"​","style":"IPY_MODEL_cbe67136d9bc4ee69b6c1966f40b7fe9","value":"Downloading: 100%"}},"cac1fbe6eb0745fd9b212021bbd8dfc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c57fc3d6a90a48d68227eebe1777b3b4","max":423498558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec6f3cbcba2e4def9f68a3d2cf747f9d","value":423498558}},"18e77e4ce6d04ffbbb0900624cd07c3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc65220fde9a4347921c99ae15c42201","placeholder":"​","style":"IPY_MODEL_e081e72541ad447e9d4c8f10ab6b0326","value":" 404M/404M [00:07&lt;00:00, 62.2MB/s]"}},"daaef1b62946476ea60537ff81a50722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd39c8859fa349f3a4b75e0dd168dfa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbe67136d9bc4ee69b6c1966f40b7fe9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c57fc3d6a90a48d68227eebe1777b3b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec6f3cbcba2e4def9f68a3d2cf747f9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc65220fde9a4347921c99ae15c42201":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e081e72541ad447e9d4c8f10ab6b0326":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}