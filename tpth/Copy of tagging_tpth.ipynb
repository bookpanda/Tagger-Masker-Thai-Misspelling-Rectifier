{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12094,"status":"ok","timestamp":1674135179000,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"IwEI9jZ9yb8G","outputId":"290003f0-d7e7-4e07-f702-665a0e93a866"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.15.0\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (2022.6.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (3.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (2.25.1)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.15.0) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (4.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.15.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.15.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.15.0) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=083cbd8d8268f079b5e2d9878d6f61a909d378a39aea428264a38b34dae3df17\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.10.3 transformers-4.15.0\n"]}],"source":["# BERT\n","!pip install transformers==4.15.0 sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2922,"status":"ok","timestamp":1674135181904,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"YbwI_6aWu49u"},"outputs":[],"source":["import pandas as pd\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch \n","import numpy as np\n","from transformers import BertTokenizerFast, BertForTokenClassification, AutoTokenizer\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from torch.optim import SGD\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"rcSzaFGhvuna"},"source":["# Read CSV Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79209,"status":"ok","timestamp":1674135261108,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"_S6FWxqDicRZ","outputId":"28ebf265-c7f5-46df-c316-c9c29172bbf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":16274,"status":"ok","timestamp":1674135280320,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"f8cnvMNLu7CO","outputId":"9a11f529-7a7f-4cde-ead8-6835edae5041"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    text  \\\n","42885  {'input_ids': [[tensor(5), tensor(10), tensor(...   \n","42886  {'input_ids': [[tensor(5), tensor(10), tensor(...   \n","42887  {'input_ids': [[tensor(5), tensor(10), tensor(...   \n","42888  {'input_ids': [[tensor(5), tensor(13276), tens...   \n","42889  {'input_ids': [[tensor(5), tensor(10), tensor(...   \n","\n","                                                  labels  \n","42885  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n","42886  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n","42887  [tensor(0), tensor(0), tensor(1), tensor(1), t...  \n","42888  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n","42889  [tensor(0), tensor(0), tensor(1), tensor(1), t...  "],"text/html":["\n","  <div id=\"df-28b91339-5edb-4da6-8fa5-a2575853c5d7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>42885</th>\n","      <td>{'input_ids': [[tensor(5), tensor(10), tensor(...</td>\n","      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n","    </tr>\n","    <tr>\n","      <th>42886</th>\n","      <td>{'input_ids': [[tensor(5), tensor(10), tensor(...</td>\n","      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n","    </tr>\n","    <tr>\n","      <th>42887</th>\n","      <td>{'input_ids': [[tensor(5), tensor(10), tensor(...</td>\n","      <td>[tensor(0), tensor(0), tensor(1), tensor(1), t...</td>\n","    </tr>\n","    <tr>\n","      <th>42888</th>\n","      <td>{'input_ids': [[tensor(5), tensor(13276), tens...</td>\n","      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n","    </tr>\n","    <tr>\n","      <th>42889</th>\n","      <td>{'input_ids': [[tensor(5), tensor(10), tensor(...</td>\n","      <td>[tensor(0), tensor(0), tensor(1), tensor(1), t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28b91339-5edb-4da6-8fa5-a2575853c5d7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-28b91339-5edb-4da6-8fa5-a2575853c5d7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-28b91339-5edb-4da6-8fa5-a2575853c5d7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["df = pickle.load(open('drive/MyDrive/AIBuilders/tpth/ner_ds_40k_nova.pkl', 'rb'))\n","df.tail()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1674135280321,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"HUdcW2Rbv-5d"},"outputs":[],"source":["df_test = df[37890:]\n","df = df[:37890]\n","df_train, df_val = np.split(df.sample(frac=1, random_state=42),\n","                            [int(.9 * len(df))])"]},{"cell_type":"code","source":["print(len(df_train), len(df_val))"],"metadata":{"id":"B0ym4iwGTIDh","executionInfo":{"status":"ok","timestamp":1661922498527,"user_tz":-420,"elapsed":2,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}},"outputId":"533176e0-d966-423f-c92d-0d66a69f0aa4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34101 3789\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1657010111560,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"MzZEQxj94gwC","outputId":"e60cf6f8-8b1f-4480-96ae-ae48eb7c410c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    1,    1,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100])"]},"metadata":{},"execution_count":17}],"source":["df.iloc[0]['labels']"]},{"cell_type":"markdown","metadata":{"id":"mosLtuxwv2hm"},"source":["# Initialize Tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VdHlzKiIvyI-","executionInfo":{"status":"ok","timestamp":1674135565427,"user_tz":-420,"elapsed":285116,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["tokenizer_th = pickle.load(open('drive/MyDrive/AIBuilders/tpth/tokenizer_40k_nova.pkl', 'rb'))"]},{"cell_type":"markdown","metadata":{"id":"ZwRa6UmDv6g4"},"source":["# Create Dataset Class"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uz8ru0XEv0yd","executionInfo":{"status":"ok","timestamp":1674135565428,"user_tz":-420,"elapsed":6,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["class DataSequence(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","\n","        # lb = [i.split() for i in df['labels'].values.tolist()]\n","        # txt = df['text'].values.tolist()\n","        # self.texts = [tokenizer_th(str(i),\n","        #                        padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n","        # self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n","        self.texts = [df.iloc[i]['text'] for i in range(df.shape[0])]\n","        self.labels = [df.iloc[i]['labels'] for i in range(df.shape[0])]\n","\n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_batch_data(self, idx):\n","\n","        return self.texts[idx]\n","\n","    def get_batch_labels(self, idx):\n","\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","\n","        batch_data = self.get_batch_data(idx)\n","        batch_labels = self.get_batch_labels(idx)\n","\n","        return batch_data, batch_labels\n"]},{"cell_type":"markdown","metadata":{"id":"BY7BgOzRwBeQ"},"source":["# Split Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3261,"status":"ok","timestamp":1656862294284,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"sDVQGS1YVZtn","outputId":"ed9fc3b3-1419-4c69-ac79-b3b584c415be"},"outputs":[{"data":{"text/plain":["({'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0]]),\n","  'input_ids': tensor([[    5,    10, 11254,    50, 30987,    10,    61,   234,   176,    20,\n","            2004,    83,  6080,  3287,    88,    70,    10, 11128,  3841,  5742,\n","            8235,  2417,   122,  5742, 13394,   437, 25372,    10,   153,  5540,\n","            2693, 26946,  2700,    23,  1063,   895,  5716,   674,   279, 10936,\n","              21,  2860, 30987,    10,    61,   780,  8685,    15,   185,  3206,\n","             169,   431, 30401,    10,   122, 10437,   207,   706,   202,    10,\n","            5464, 18962,   102, 32729,    10,  2663,   839,   700,   202,    10,\n","             883,    50, 30987,    10,    61,  1073, 10454,    70,    10, 29251,\n","              10,  5742,   165,     6,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","               1,     1]])},\n"," tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100]))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["asdf = DataSequence(df_train)\n","asdf.__getitem__(1)"]},{"cell_type":"markdown","metadata":{"id":"N12N752nwGin"},"source":["# Build Model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gDSbpnWtwEmo","executionInfo":{"status":"ok","timestamp":1674135565428,"user_tz":-420,"elapsed":4,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[],"source":["class BertModel(torch.nn.Module):\n","\n","    def __init__(self):\n","\n","        super(BertModel, self).__init__()\n","        self.bert = BertForTokenClassification.from_pretrained('airesearch/wangchanberta-base-att-spm-uncased', num_labels=2)\n","        self.bert.resize_token_embeddings(len(tokenizer_th))\n","\n","    def forward(self, input_id, mask, label):\n","\n","        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n","\n","        return output"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["397a57e6ad2d40dea47de7c2717caa32","2f9758d1e426405fa221042ea3081799","b70e5275f0b942b1a3e0b4b322261137","58e4b66273264f8fabeef42289b2c2ab","9edd10dc73a043cca62d0291d71efde1","592ff20da28d45958431c6cde89cb86c","6a4681d8adae4376ab3522ec97b063df","f1b099ad729b449f85522843e4792569","ed9fae2216924b97926957625e72321f","816c2da8b33e4657bde4caa018a80d43","07edff509e1b4a35a70dd07436e36ba8","d3b1ca3576c64930826202463e801552","d5aa2016b8ae4d8ba5ab39b36eb91540","bc68b18f36924b6fb79e45ca8955cf7d","977c8f47519744f4bf096538f4480c09","e8adb81f492a4592917919117fb30688","6fcbb9419f8a414ea5be448e41f3b74d","3a5740deb0394b6fa66cd568658ee5f0","95beb0040793487fadf4fde5dd6458c7","e5452d8471d643a9bcb6c2e96bf71ed0","aca116c1e0d547cc8380acc07e6c0a4a","c94433f4647e419788d8e9828a0d3a12"]},"executionInfo":{"elapsed":19074,"status":"ok","timestamp":1674135584498,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"Epie4o66Reup","outputId":"10445fd3-70a2-4c87-b733-6ed5e0690c74"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397a57e6ad2d40dea47de7c2717caa32"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using a model of type camembert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/404M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b1ca3576c64930826202463e801552"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing BertForTokenClassification: ['roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'lm_head.decoder.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'lm_head.layer_norm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'lm_head.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'classifier.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'classifier.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (bert): BertForTokenClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(33660, 768)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":9}],"source":["model = BertModel()\n","# model = BertForTokenClassification.from_pretrained('airesearch/wangchanberta-base-att-spm-uncased', num_labels=2)\n","# model.resize_token_embeddings(len(tokenizer_th))\n","\n","# FILE = \"drive/MyDrive/AIBuilders/tagging.pth\"\n","# FILE = \"drive/MyDrive/AIBuilders/tagging_nova_75.pth\"\n","FILE = \"drive/MyDrive/AIBuilders/tpth/tagging_tpth_200.pth\"\n","model.load_state_dict(torch.load(FILE, map_location=torch.device('cpu')))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1655650258480,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"sqMWGt1MnH_w","outputId":"7bb2640f-705c-4d57-d681-d261b7569c04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login() # เอาไว้โยนโมเดลขึ้น hugging face ได้เลย"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1655651567919,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"vDm03Vdhopos","outputId":"c8a6b85a-17ab-4165-8961-005911abae32"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["from transformers import TrainingArguments\n","\n","model_checkpoint = \"airesearch/wangchanberta-base-att-spm-uncased\"\n","\n","batch_size = 32\n","# Show the training loss with every epoch\n","logging_steps = len(df_train) // batch_size\n","model_name = model_checkpoint.split(\"/\")[-1]\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_name}-tagging\",\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    push_to_hub=True,\n","    num_train_epochs = 75,\n","    fp16=True, # สำหรับคนใช้ GPU\n","    logging_steps=logging_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6136,"status":"ok","timestamp":1655651574576,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"LbTQONglp2Su","outputId":"dc5801ce-a374-448d-d1ce-f6ac853af042"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/wangchanberta-base-att-spm-uncased-tagging is already a clone of https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging. Make sure you pull the latest changes with `repo.git_pull()`.\n","Using amp half precision backend\n"]}],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=df_train,\n","    eval_dataset=df_test,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["00fd55d320cc43daa84d9128f243e217","4f657ca9ea6f40d596e438a8397a6164","37c30ad614fa4d7e92f0f56c8d83e03e","41ba46ad8168442ba5c97804e64c537c","8150574dae9a4e8e9f01c943f81cf48a","91f2805cdea443579516a4fd448bcec5","deb791c8e7bc4b49be62c6bf8272cf21","b862d06dab204371b1d19697fee092a9","755dc07041ef41808382ce856b28c114","5c54bd59f2cc44d2a2a31690e03fbda4","a1ad8789a4324143a7a442c111b18a2a","d627bbabb1bb4e10b6cf745d65075366","53cfe0ced88745219f05407caceea7db","89fc3fa575f14c78a6f915cba663e665","c62c380001fe400bbf780d3a322a08a5","a7f6d4c051f249748a18b68cad564765","a525cda209b64daba6d85f4f75fc0e50","cd27b78f3b694baeaab64120681bedf3","8585b55f4bb44dce8cc5fba495d4c48d","5c9139ad997e4421bce56c88197e3dc5","fcb2a9558f4c457fb0402fec760dd7f3","fbe141af7c004fef9d04f410eb6901b7"]},"executionInfo":{"elapsed":417581,"status":"ok","timestamp":1655651996877,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"nlabWhS4n_Gf","outputId":"f8b869f4-8d49-4063-f6bb-2a2d2df40cd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to wangchanberta-base-att-spm-uncased-tagging\n","Configuration saved in wangchanberta-base-att-spm-uncased-tagging/config.json\n","Model weights saved in wangchanberta-base-att-spm-uncased-tagging/pytorch_model.bin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00fd55d320cc43daa84d9128f243e217","version_major":2,"version_minor":0},"text/plain":["Upload file pytorch_model.bin:   0%|          | 3.34k/425M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d627bbabb1bb4e10b6cf745d65075366","version_major":2,"version_minor":0},"text/plain":["Upload file training_args.bin: 100%|##########| 2.92k/2.92k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["To https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging\n","   3c3ed19..daa973a  main -> main\n","\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Token Classification', 'type': 'token-classification'}}\n","To https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging\n","   daa973a..712de4d  main -> main\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/bookpanda/wangchanberta-base-att-spm-uncased-tagging/commit/daa973a5e4f9d4068f6ffab57f3a3af8d9f1b64a'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":637,"status":"ok","timestamp":1657010613041,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"emyGME48RA4a","outputId":"f0114227-63ee-4516-d0d3-b62703e75544"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (bert): BertForTokenClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(33660, 768)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":30}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"C6zDk8yQwLqq"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0jSoWhNwJpN"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","def train_loop(model, df_train, df_val):\n","\n","    train_dataset = DataSequence(df_train)\n","    val_dataset = DataSequence(df_val)\n","\n","    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    best_acc = 0\n","    best_loss = 1000\n","    \n","    for epoch_num in range(EPOCHS):\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        model.train()\n","        \n","        for train_data, train_label in tqdm(train_dataloader):\n","            train_label = train_label[0].to(device)\n","            mask = train_data['attention_mask'][0].to(device)\n","            input_id = train_data['input_ids'][0].to(device)\n","            optimizer.zero_grad()\n","            loss, logits = model(input_id, mask, train_label)\n","\n","            logits_clean = logits[0][train_label != -100]\n","            label_clean = train_label[train_label != -100]\n","            predictions = logits_clean.argmax(dim=1)\n","            pred_len = len(predictions)\n","            for i in range(pred_len):\n","                if predictions[i] == 0:\n","                    predictions[i] = -100\n","            \n","            # print(predictions)\n","            # print(label_clean)\n","            numer = 0\n","            denom = 0\n","            for i in range(pred_len):\n","                if label_clean[i] == 1:\n","                    denom += 1\n","                    if predictions[i] == 1:\n","                        numer += 1\n","                elif label_clean[i] == 0 and predictions[i] == 1:\n","                    denom += 1\n","\n","            if denom == 0:\n","              acc = 0\n","            else:\n","              acc = float(numer)/float(denom)\n","            # print(f\"train acc: {acc}\")\n","            total_acc_train += acc * BATCH_SIZE\n","            total_loss_train += loss.item() * BATCH_SIZE\n","\n","            loss.backward()\n","            optimizer.step()\n","        \n","        model.eval()\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        for val_data, val_label in val_dataloader:\n","\n","            val_label = val_label[0].to(device)\n","            mask = val_data['attention_mask'][0].to(device)\n","\n","            input_id = val_data['input_ids'][0].to(device)\n","\n","            loss, logits = model(input_id, mask, val_label)\n","\n","            logits_clean = logits[0][val_label != -100]\n","            label_clean = val_label[val_label != -100]\n","            predictions = logits_clean.argmax(dim=1)   \n","            pred_len = len(predictions)\n","            for i in range(pred_len):\n","                if predictions[i] == 0:\n","                    predictions[i] = -100       \n","\n","            numer = 0\n","            denom = 0\n","            for i in range(pred_len):\n","                if label_clean[i] == 1:\n","                    denom += 1\n","                    if predictions[i] == 1:\n","                        numer += 1\n","                elif label_clean[i] == 0 and predictions[i] == 1:\n","                    denom += 1\n","        \n","            if denom == 0:\n","              acc = 0\n","            else:\n","              acc = float(numer)/float(denom)\n","            # print(f\"valid acc: {acc}\")\n","            total_acc_val += acc * BATCH_SIZE\n","            total_loss_val += loss.item() * BATCH_SIZE\n","\n","        val_accuracy = total_acc_val / len(df_val)\n","        val_loss = total_loss_val / len(df_val)\n","\n","        print(\n","            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n","\n","LEARNING_RATE = 1e-3\n","EPOCHS = 100\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzqsWQ0xOXU1","outputId":"ee0cafe0-7aba-43d1-a906-af20700e2581","executionInfo":{"status":"ok","timestamp":1657039046284,"user_tz":-420,"elapsed":3224976,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 1066/1066 [02:09<00:00,  8.21it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 1 | Loss:  0.132 | Accuracy:  0.454 | Val_Loss:  0.138 | Accuracy:  0.479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.18it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 2 | Loss:  0.134 | Accuracy:  0.459 | Val_Loss:  0.136 | Accuracy:  0.471\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.23it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 3 | Loss:  0.130 | Accuracy:  0.464 | Val_Loss:  0.140 | Accuracy:  0.457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 4 | Loss:  0.131 | Accuracy:  0.476 | Val_Loss:  0.135 | Accuracy:  0.483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.21it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 5 | Loss:  0.132 | Accuracy:  0.469 | Val_Loss:  0.137 | Accuracy:  0.465\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.17it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 6 | Loss:  0.133 | Accuracy:  0.470 | Val_Loss:  0.143 | Accuracy:  0.455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 7 | Loss:  0.135 | Accuracy:  0.460 | Val_Loss:  0.135 | Accuracy:  0.479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 8 | Loss:  0.133 | Accuracy:  0.458 | Val_Loss:  0.134 | Accuracy:  0.485\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 9 | Loss:  0.137 | Accuracy:  0.457 | Val_Loss:  0.134 | Accuracy:  0.505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 10 | Loss:  0.136 | Accuracy:  0.467 | Val_Loss:  0.133 | Accuracy:  0.500\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 11 | Loss:  0.133 | Accuracy:  0.454 | Val_Loss:  0.134 | Accuracy:  0.477\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.23it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 12 | Loss:  0.133 | Accuracy:  0.472 | Val_Loss:  0.129 | Accuracy:  0.515\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 13 | Loss:  0.138 | Accuracy:  0.455 | Val_Loss:  0.134 | Accuracy:  0.475\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 14 | Loss:  0.128 | Accuracy:  0.463 | Val_Loss:  0.130 | Accuracy:  0.488\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.23it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 15 | Loss:  0.135 | Accuracy:  0.468 | Val_Loss:  0.132 | Accuracy:  0.491\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 16 | Loss:  0.135 | Accuracy:  0.453 | Val_Loss:  0.139 | Accuracy:  0.460\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 17 | Loss:  0.132 | Accuracy:  0.460 | Val_Loss:  0.133 | Accuracy:  0.478\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 18 | Loss:  0.136 | Accuracy:  0.463 | Val_Loss:  0.130 | Accuracy:  0.492\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.23it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 19 | Loss:  0.128 | Accuracy:  0.476 | Val_Loss:  0.133 | Accuracy:  0.496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.23it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 20 | Loss:  0.137 | Accuracy:  0.457 | Val_Loss:  0.130 | Accuracy:  0.502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 21 | Loss:  0.134 | Accuracy:  0.470 | Val_Loss:  0.136 | Accuracy:  0.484\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.20it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 22 | Loss:  0.133 | Accuracy:  0.468 | Val_Loss:  0.135 | Accuracy:  0.472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 23 | Loss:  0.128 | Accuracy:  0.473 | Val_Loss:  0.142 | Accuracy:  0.452\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 24 | Loss:  0.130 | Accuracy:  0.466 | Val_Loss:  0.129 | Accuracy:  0.525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.23it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 25 | Loss:  0.127 | Accuracy:  0.472 | Val_Loss:  0.130 | Accuracy:  0.509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 26 | Loss:  0.129 | Accuracy:  0.467 | Val_Loss:  0.131 | Accuracy:  0.490\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 27 | Loss:  0.134 | Accuracy:  0.461 | Val_Loss:  0.128 | Accuracy:  0.504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 28 | Loss:  0.132 | Accuracy:  0.455 | Val_Loss:  0.132 | Accuracy:  0.501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.21it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 29 | Loss:  0.134 | Accuracy:  0.448 | Val_Loss:  0.129 | Accuracy:  0.499\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.18it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 30 | Loss:  0.130 | Accuracy:  0.475 | Val_Loss:  0.134 | Accuracy:  0.492\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:09<00:00,  8.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 31 | Loss:  0.131 | Accuracy:  0.473 | Val_Loss:  0.141 | Accuracy:  0.466\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:10<00:00,  8.14it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 32 | Loss:  0.135 | Accuracy:  0.468 | Val_Loss:  0.129 | Accuracy:  0.508\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.11it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 33 | Loss:  0.132 | Accuracy:  0.480 | Val_Loss:  0.128 | Accuracy:  0.510\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.09it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 34 | Loss:  0.132 | Accuracy:  0.467 | Val_Loss:  0.131 | Accuracy:  0.495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.09it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 35 | Loss:  0.131 | Accuracy:  0.490 | Val_Loss:  0.128 | Accuracy:  0.524\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 36 | Loss:  0.131 | Accuracy:  0.468 | Val_Loss:  0.129 | Accuracy:  0.506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 37 | Loss:  0.131 | Accuracy:  0.464 | Val_Loss:  0.129 | Accuracy:  0.502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 38 | Loss:  0.129 | Accuracy:  0.475 | Val_Loss:  0.129 | Accuracy:  0.508\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 39 | Loss:  0.132 | Accuracy:  0.459 | Val_Loss:  0.132 | Accuracy:  0.503\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 40 | Loss:  0.130 | Accuracy:  0.475 | Val_Loss:  0.129 | Accuracy:  0.519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 41 | Loss:  0.126 | Accuracy:  0.475 | Val_Loss:  0.127 | Accuracy:  0.525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 42 | Loss:  0.130 | Accuracy:  0.484 | Val_Loss:  0.130 | Accuracy:  0.497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 43 | Loss:  0.127 | Accuracy:  0.473 | Val_Loss:  0.129 | Accuracy:  0.513\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 44 | Loss:  0.133 | Accuracy:  0.469 | Val_Loss:  0.129 | Accuracy:  0.509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 45 | Loss:  0.130 | Accuracy:  0.480 | Val_Loss:  0.128 | Accuracy:  0.527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.04it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 46 | Loss:  0.132 | Accuracy:  0.475 | Val_Loss:  0.138 | Accuracy:  0.456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 47 | Loss:  0.127 | Accuracy:  0.465 | Val_Loss:  0.136 | Accuracy:  0.467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 48 | Loss:  0.124 | Accuracy:  0.475 | Val_Loss:  0.128 | Accuracy:  0.521\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 49 | Loss:  0.128 | Accuracy:  0.478 | Val_Loss:  0.135 | Accuracy:  0.472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 50 | Loss:  0.130 | Accuracy:  0.482 | Val_Loss:  0.132 | Accuracy:  0.476\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 51 | Loss:  0.132 | Accuracy:  0.453 | Val_Loss:  0.128 | Accuracy:  0.504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 52 | Loss:  0.132 | Accuracy:  0.478 | Val_Loss:  0.126 | Accuracy:  0.520\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 53 | Loss:  0.128 | Accuracy:  0.481 | Val_Loss:  0.128 | Accuracy:  0.530\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 54 | Loss:  0.128 | Accuracy:  0.492 | Val_Loss:  0.126 | Accuracy:  0.510\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 55 | Loss:  0.122 | Accuracy:  0.485 | Val_Loss:  0.136 | Accuracy:  0.462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 56 | Loss:  0.127 | Accuracy:  0.483 | Val_Loss:  0.130 | Accuracy:  0.496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 57 | Loss:  0.130 | Accuracy:  0.471 | Val_Loss:  0.129 | Accuracy:  0.506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 58 | Loss:  0.129 | Accuracy:  0.453 | Val_Loss:  0.127 | Accuracy:  0.495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 59 | Loss:  0.131 | Accuracy:  0.480 | Val_Loss:  0.127 | Accuracy:  0.514\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 60 | Loss:  0.122 | Accuracy:  0.477 | Val_Loss:  0.127 | Accuracy:  0.524\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 61 | Loss:  0.128 | Accuracy:  0.482 | Val_Loss:  0.126 | Accuracy:  0.528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 62 | Loss:  0.122 | Accuracy:  0.489 | Val_Loss:  0.133 | Accuracy:  0.499\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 63 | Loss:  0.127 | Accuracy:  0.479 | Val_Loss:  0.126 | Accuracy:  0.514\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 64 | Loss:  0.128 | Accuracy:  0.481 | Val_Loss:  0.144 | Accuracy:  0.446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 65 | Loss:  0.129 | Accuracy:  0.479 | Val_Loss:  0.131 | Accuracy:  0.477\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 66 | Loss:  0.126 | Accuracy:  0.488 | Val_Loss:  0.138 | Accuracy:  0.456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.04it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 67 | Loss:  0.131 | Accuracy:  0.455 | Val_Loss:  0.129 | Accuracy:  0.502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 68 | Loss:  0.124 | Accuracy:  0.493 | Val_Loss:  0.129 | Accuracy:  0.503\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 69 | Loss:  0.131 | Accuracy:  0.477 | Val_Loss:  0.127 | Accuracy:  0.511\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 70 | Loss:  0.125 | Accuracy:  0.474 | Val_Loss:  0.136 | Accuracy:  0.476\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.03it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 71 | Loss:  0.130 | Accuracy:  0.478 | Val_Loss:  0.127 | Accuracy:  0.516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 72 | Loss:  0.124 | Accuracy:  0.488 | Val_Loss:  0.128 | Accuracy:  0.501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 73 | Loss:  0.132 | Accuracy:  0.478 | Val_Loss:  0.130 | Accuracy:  0.541\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 74 | Loss:  0.127 | Accuracy:  0.476 | Val_Loss:  0.125 | Accuracy:  0.531\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 75 | Loss:  0.125 | Accuracy:  0.482 | Val_Loss:  0.125 | Accuracy:  0.517\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1066/1066 [02:13<00:00,  8.01it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 76 | Loss:  0.128 | Accuracy:  0.480 | Val_Loss:  0.126 | Accuracy:  0.509\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 77 | Loss:  0.128 | Accuracy:  0.477 | Val_Loss:  0.127 | Accuracy:  0.500\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 78 | Loss:  0.131 | Accuracy:  0.484 | Val_Loss:  0.124 | Accuracy:  0.519\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 79 | Loss:  0.123 | Accuracy:  0.495 | Val_Loss:  0.130 | Accuracy:  0.503\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 80 | Loss:  0.126 | Accuracy:  0.492 | Val_Loss:  0.126 | Accuracy:  0.503\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 81 | Loss:  0.120 | Accuracy:  0.491 | Val_Loss:  0.128 | Accuracy:  0.534\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 82 | Loss:  0.129 | Accuracy:  0.484 | Val_Loss:  0.127 | Accuracy:  0.538\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 83 | Loss:  0.127 | Accuracy:  0.484 | Val_Loss:  0.124 | Accuracy:  0.520\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:13<00:00,  8.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 84 | Loss:  0.127 | Accuracy:  0.492 | Val_Loss:  0.125 | Accuracy:  0.506\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 85 | Loss:  0.128 | Accuracy:  0.479 | Val_Loss:  0.124 | Accuracy:  0.535\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 86 | Loss:  0.127 | Accuracy:  0.474 | Val_Loss:  0.127 | Accuracy:  0.523\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 87 | Loss:  0.129 | Accuracy:  0.479 | Val_Loss:  0.124 | Accuracy:  0.527\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 88 | Loss:  0.126 | Accuracy:  0.476 | Val_Loss:  0.125 | Accuracy:  0.506\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 89 | Loss:  0.126 | Accuracy:  0.490 | Val_Loss:  0.129 | Accuracy:  0.491\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 90 | Loss:  0.130 | Accuracy:  0.500 | Val_Loss:  0.125 | Accuracy:  0.512\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 91 | Loss:  0.127 | Accuracy:  0.467 | Val_Loss:  0.129 | Accuracy:  0.509\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 92 | Loss:  0.121 | Accuracy:  0.491 | Val_Loss:  0.132 | Accuracy:  0.500\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 93 | Loss:  0.124 | Accuracy:  0.500 | Val_Loss:  0.125 | Accuracy:  0.507\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 94 | Loss:  0.124 | Accuracy:  0.477 | Val_Loss:  0.130 | Accuracy:  0.493\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 95 | Loss:  0.124 | Accuracy:  0.482 | Val_Loss:  0.125 | Accuracy:  0.515\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 96 | Loss:  0.127 | Accuracy:  0.480 | Val_Loss:  0.124 | Accuracy:  0.540\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:11<00:00,  8.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 97 | Loss:  0.122 | Accuracy:  0.490 | Val_Loss:  0.135 | Accuracy:  0.484\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 98 | Loss:  0.124 | Accuracy:  0.482 | Val_Loss:  0.124 | Accuracy:  0.513\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 99 | Loss:  0.124 | Accuracy:  0.495 | Val_Loss:  0.130 | Accuracy:  0.515\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1066/1066 [02:12<00:00,  8.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 100 | Loss:  0.124 | Accuracy:  0.490 | Val_Loss:  0.124 | Accuracy:  0.515\n"]}],"source":["train_loop(model, df_train, df_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvzoFdbk8duB"},"outputs":[],"source":["FILE = \"drive/MyDrive/AIBuilders/tagging_tpth_100.pth\"\n","torch.save(model.state_dict(), FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0ebNG0yP9zU"},"outputs":[],"source":["train_loop(model, df_train, df_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXoMghwgP-no"},"outputs":[],"source":["FILE = \"drive/MyDrive/AIBuilders/tagging_tpth_200.pth\"\n","torch.save(model.state_dict(), FILE)"]},{"cell_type":"markdown","metadata":{"id":"4wQtjsdlxrQH"},"source":["# Evaluate Model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364204,"status":"ok","timestamp":1674136265286,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"hYGHTi-cwPtA","outputId":"89ad5462-a859-4ea1-ea20-891fa7a7e8d8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["=============100=================\n","=============200=================\n","=============300=================\n","=============400=================\n","=============500=================\n","=============600=================\n","=============700=================\n","=============800=================\n","=============900=================\n","=============1000=================\n","=============1100=================\n","=============1200=================\n","=============1300=================\n","=============1400=================\n","=============1500=================\n","=============1600=================\n","=============1700=================\n","=============1800=================\n","=============1900=================\n","=============2000=================\n","=============2100=================\n","=============2200=================\n","=============2300=================\n","=============2400=================\n","=============2500=================\n","=============2600=================\n","=============2700=================\n","=============2800=================\n","=============2900=================\n","=============3000=================\n","=============3100=================\n","=============3200=================\n","=============3300=================\n","=============3400=================\n","=============3500=================\n","=============3600=================\n","=============3700=================\n","=============3800=================\n","=============3900=================\n","=============4000=================\n","=============4100=================\n","=============4200=================\n","=============4300=================\n","=============4400=================\n","=============4500=================\n","=============4600=================\n","=============4700=================\n","=============4800=================\n","=============4900=================\n","=============5000=================\n","Test Accuracy:  0.463\n","0 >>> precision:  0.962    recall:  0.987    f1:  0.974\n","1 >>> precision:  0.715    recall:  0.552    f1:  0.580\n"]}],"source":["def evaluate(model, df_test):\n","\n","    test_dataset = DataSequence(df_test)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    total_acc_test = 0.0\n","\n","    cou = 0\n","    total_precision = [0,0]\n","    total_recall = [0,0]\n","    total_f1 = [0,0]\n","    for test_data, test_label in test_dataloader:\n","        # if cou == 20:\n","        #     break\n","        cou+=1\n","        if cou%100==0:\n","          print(f\"============={cou}=================\")\n","\n","        test_label = test_label[0].to(device)\n","        mask = test_data['attention_mask'][0].to(device)\n","        input_id = test_data['input_ids'][0].to(device)\n","          \n","        loss, logits = model(input_id, mask, test_label.long())\n","        # print(test_data)\n","        # print(test_label)\n","        # print(loss)\n","        # print(logits)\n","\n","        logits_clean = logits[0][test_label != -100]\n","        label_clean = test_label[test_label != -100]\n","        predictions = logits_clean.argmax(dim=1)\n","        pred_len = len(predictions)\n","        for i in range(pred_len):\n","            if predictions[i] == 0:\n","                predictions[i] = -100\n","        # print(logits_clean)\n","        # print(label_clean)\n","        # print(predictions)\n","\n","  \n","        a = tokenizer_th.convert_ids_to_tokens(input_id.squeeze())\n","        # print(f\"input: {''.join(a)}\")\n","        # print(label_clean)\n","        # print(len(input_id[0]), len(label_clean))\n","        # print(\"________________________\")\n","\n","        ids_to_labels = {0:'f', 1:'i', -100:'f'}\n","        prediction_label = [ids_to_labels[i] for i in predictions.tolist()]\n","        # print(\"PREDICTIONS\")\n","        # for i in range(len(predictions)):\n","        #   if prediction_label[i] == 'i':\n","        #     print(a[i])\n","        # print(\"LABELS\")\n","        # for i in range(len(label_clean)):\n","        #   if label_clean[i] == 1:\n","        #     print(a[i])   \n","\n","        # acc = (predictions == label_clean).float().mean()\n","\n","        # print(predictions, label_clean)\n","        TP = [0,0]\n","        FP = [0,0]\n","        TN = [0,0]\n","        FN = [0,0]\n","        for i in range(pred_len):\n","          if predictions[i] == 1 and label_clean[i] == 1:\n","            TP[1] += 1\n","            TN[0] += 1\n","          elif predictions[i] == -100 and label_clean[i] == 1:\n","            FP[0] += 1\n","            FN[1] += 1\n","          elif predictions[i] == 1 and label_clean[i] == 0:\n","            FP[1] += 1\n","            FN[0] += 1\n","          elif predictions[i] == -100 and label_clean[i] == 0:\n","            TP[0] += 1\n","            TN[1] += 1\n","\n","        precision = [float(TP[0]) / float(TP[0]+FP[0]) if TP[0]+FP[0] > 0 else 0, float(TP[1]) / float(TP[1]+FP[1]) if TP[1]+FP[1] > 0 else 0]\n","        recall = [float(TP[0]) / float(TP[0]+FN[0]) if TP[0]+FN[0] > 0 else 0, float(TP[1]) / float(TP[1]+FN[1]) if TP[1]+FN[1] > 0 else 0]\n","        f1 = [float(2*precision[0]*recall[0]) / float(precision[0] + recall[0]) if precision[0]+recall[0] > 0 else 0, float(2*precision[1]*recall[1]) / float(precision[1] + recall[1]) if precision[1]+recall[1] > 0 else 0]\n","        # print(f\"0 >>> TP:{TP[0]}   TN:{TN[0]}   FP:{FP[0]}   FN:{FN[0]}   precision:{precision[0]}    recall:{recall[0]}    f1:{f1[0]}\")\n","        # print(f\"1 >>> TP:{TP[1]}   TN:{TN[1]}   FP:{FP[1]}   FN:{FN[1]}   precision:{precision[1]}    recall:{recall[1]}    f1:{f1[1]}\")\n","        total_precision[0] += precision[0]\n","        total_precision[1] += precision[1]\n","        total_recall[0] += recall[0]\n","        total_recall[1] += recall[1]\n","        total_f1[0] += f1[0]\n","        total_f1[1] += f1[1]\n","\n","        numer = 0\n","        denom = 0\n","        for i in range(pred_len):\n","            if label_clean[i] == 1:\n","                denom += 1\n","                if predictions[i] == 1:\n","                    numer += 1\n","            elif label_clean[i] == 0 and predictions[i] == 1:\n","                denom += 1\n","        \n","        acc = float(numer)/float(denom) if denom > 0 else 0\n","        # print(f\"ACC: {acc}\")\n","        # print(\"-----------------------------------\")\n","        total_acc_test += acc\n","\n","    val_accuracy = total_acc_test / len(df_test)\n","    # print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n","    print(f'Test Accuracy: {total_acc_test / cou: .3f}')\n","    print(f'0 >>> precision: {total_precision[0] / cou: .3f}    recall: {total_recall[0] / cou: .3f}    f1: {total_f1[0] / cou: .3f}')\n","    print(f'1 >>> precision: {total_precision[1] / cou: .3f}    recall: {total_recall[1] / cou: .3f}    f1: {total_f1[1] / cou: .3f}')\n","\n","evaluate(model, df_test)"]},{"cell_type":"markdown","metadata":{"id":"EyK8C2_fxvk6"},"source":["# Predict One Sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1655188472122,"user":{"displayName":"Idhibhat Pankam","userId":"11974197125920887983"},"user_tz":-420},"id":"T99Q2GKQxtzK","outputId":"7029341a-d823-428f-aff2-b883a207b02f"},"outputs":[{"name":"stdout","output_type":"stream","text":["ids_to_tokens: ['<s>', '▁', 'ประเทศ', 'เรา', 'ผลิต', 'และ', 'ส่งออก', 'ยา', '▁', 'สูบ', 'เยอะ', 'สุดในโลก', 'จิง', 'ป่าว', 'คับ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","['f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'i', 'i', 'i']\n","['<s>', '▁', 'ประเทศ', 'เรา', 'ผลิต', 'และ', 'ส่งออก', 'ยา', '▁', 'สูบ', 'เยอะ', 'สุดในโลก', 'จิง', 'ป่าว', 'คับ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","สุดในโลก จิง ป่าว\n","จิง ป่าว คับ\n","ป่าว คับ </s>\n"]}],"source":["def align_word_ids(texts):\n","  \n","    tokenized_inputs = tokenizer_th(texts, padding='max_length', max_length=512, truncation=True)\n","    c = tokenizer_th.convert_ids_to_tokens(tokenized_inputs.input_ids)\n","    print(f\"ids_to_tokens: {c}\")\n","\n","    word_ids = tokenized_inputs.word_ids()\n","    previous_word_idx = None\n","    label_ids = []\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        else:\n","            try:\n","              label_ids.append(2)\n","            except:\n","                label_ids.append(-100)\n","\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","\n","def evaluate_one_text(model, sentence):\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    text = tokenizer_th(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","\n","    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n","   \n","    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n","    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n","   \n","    logits = model(input_id, mask, None)\n","    # print(input_id)\n","    # print(mask)\n","    # print(label_ids)\n","    logits_clean = logits[0][label_ids != -100]\n","    # print(logits_clean)\n","    # print(len(logits[0][0]), len(logits_clean))\n","\n","    predictions = logits_clean.argmax(dim=1).tolist()\n","    ids_to_labels = {0:'f', 1:'i'}\n","    prediction_label = [ids_to_labels[i] for i in predictions]\n","    print(prediction_label)\n","\n","    tokenized_inputs = tokenizer_th(sentence, padding='max_length', max_length=512, truncation=True)\n","    c = tokenizer_th.convert_ids_to_tokens(tokenized_inputs.input_ids)\n","    print(c)\n","    for i, j in enumerate(prediction_label):\n","        if j == 'i':\n","            print(c[i], c[i+1], c[i+2])\n","            \n","evaluate_one_text(model, 'ประเทศเราผลิตและส่งออกยาสูบเยอะสุดในโลกจิงป่าวคับ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-o6XaM5xzLR"},"outputs":[],"source":["FILE = \"tagging.pth\"\n","torch.save(model.bert.state_dict(), FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhWcAJXUVNU-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00fd55d320cc43daa84d9128f243e217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f657ca9ea6f40d596e438a8397a6164","IPY_MODEL_37c30ad614fa4d7e92f0f56c8d83e03e","IPY_MODEL_41ba46ad8168442ba5c97804e64c537c"],"layout":"IPY_MODEL_8150574dae9a4e8e9f01c943f81cf48a"}},"37c30ad614fa4d7e92f0f56c8d83e03e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b862d06dab204371b1d19697fee092a9","max":445293041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_755dc07041ef41808382ce856b28c114","value":445293041}},"41ba46ad8168442ba5c97804e64c537c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c54bd59f2cc44d2a2a31690e03fbda4","placeholder":"​","style":"IPY_MODEL_a1ad8789a4324143a7a442c111b18a2a","value":" 425M/425M [06:36&lt;00:00, 697kB/s]"}},"4f657ca9ea6f40d596e438a8397a6164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91f2805cdea443579516a4fd448bcec5","placeholder":"​","style":"IPY_MODEL_deb791c8e7bc4b49be62c6bf8272cf21","value":"Upload file pytorch_model.bin: 100%"}},"53cfe0ced88745219f05407caceea7db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a525cda209b64daba6d85f4f75fc0e50","placeholder":"​","style":"IPY_MODEL_cd27b78f3b694baeaab64120681bedf3","value":"Upload file training_args.bin: 100%"}},"5c54bd59f2cc44d2a2a31690e03fbda4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c9139ad997e4421bce56c88197e3dc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"755dc07041ef41808382ce856b28c114":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8150574dae9a4e8e9f01c943f81cf48a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8585b55f4bb44dce8cc5fba495d4c48d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89fc3fa575f14c78a6f915cba663e665":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8585b55f4bb44dce8cc5fba495d4c48d","max":2991,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c9139ad997e4421bce56c88197e3dc5","value":2991}},"91f2805cdea443579516a4fd448bcec5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1ad8789a4324143a7a442c111b18a2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a525cda209b64daba6d85f4f75fc0e50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f6d4c051f249748a18b68cad564765":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b862d06dab204371b1d19697fee092a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62c380001fe400bbf780d3a322a08a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcb2a9558f4c457fb0402fec760dd7f3","placeholder":"​","style":"IPY_MODEL_fbe141af7c004fef9d04f410eb6901b7","value":" 2.92k/2.92k [06:35&lt;?, ?B/s]"}},"cd27b78f3b694baeaab64120681bedf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d627bbabb1bb4e10b6cf745d65075366":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53cfe0ced88745219f05407caceea7db","IPY_MODEL_89fc3fa575f14c78a6f915cba663e665","IPY_MODEL_c62c380001fe400bbf780d3a322a08a5"],"layout":"IPY_MODEL_a7f6d4c051f249748a18b68cad564765"}},"deb791c8e7bc4b49be62c6bf8272cf21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbe141af7c004fef9d04f410eb6901b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcb2a9558f4c457fb0402fec760dd7f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"397a57e6ad2d40dea47de7c2717caa32":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f9758d1e426405fa221042ea3081799","IPY_MODEL_b70e5275f0b942b1a3e0b4b322261137","IPY_MODEL_58e4b66273264f8fabeef42289b2c2ab"],"layout":"IPY_MODEL_9edd10dc73a043cca62d0291d71efde1"}},"2f9758d1e426405fa221042ea3081799":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_592ff20da28d45958431c6cde89cb86c","placeholder":"​","style":"IPY_MODEL_6a4681d8adae4376ab3522ec97b063df","value":"Downloading: 100%"}},"b70e5275f0b942b1a3e0b4b322261137":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1b099ad729b449f85522843e4792569","max":546,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed9fae2216924b97926957625e72321f","value":546}},"58e4b66273264f8fabeef42289b2c2ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_816c2da8b33e4657bde4caa018a80d43","placeholder":"​","style":"IPY_MODEL_07edff509e1b4a35a70dd07436e36ba8","value":" 546/546 [00:00&lt;00:00, 31.7kB/s]"}},"9edd10dc73a043cca62d0291d71efde1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"592ff20da28d45958431c6cde89cb86c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a4681d8adae4376ab3522ec97b063df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1b099ad729b449f85522843e4792569":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9fae2216924b97926957625e72321f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"816c2da8b33e4657bde4caa018a80d43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07edff509e1b4a35a70dd07436e36ba8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3b1ca3576c64930826202463e801552":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5aa2016b8ae4d8ba5ab39b36eb91540","IPY_MODEL_bc68b18f36924b6fb79e45ca8955cf7d","IPY_MODEL_977c8f47519744f4bf096538f4480c09"],"layout":"IPY_MODEL_e8adb81f492a4592917919117fb30688"}},"d5aa2016b8ae4d8ba5ab39b36eb91540":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fcbb9419f8a414ea5be448e41f3b74d","placeholder":"​","style":"IPY_MODEL_3a5740deb0394b6fa66cd568658ee5f0","value":"Downloading: 100%"}},"bc68b18f36924b6fb79e45ca8955cf7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95beb0040793487fadf4fde5dd6458c7","max":423498558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5452d8471d643a9bcb6c2e96bf71ed0","value":423498558}},"977c8f47519744f4bf096538f4480c09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aca116c1e0d547cc8380acc07e6c0a4a","placeholder":"​","style":"IPY_MODEL_c94433f4647e419788d8e9828a0d3a12","value":" 404M/404M [00:05&lt;00:00, 77.4MB/s]"}},"e8adb81f492a4592917919117fb30688":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fcbb9419f8a414ea5be448e41f3b74d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5740deb0394b6fa66cd568658ee5f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95beb0040793487fadf4fde5dd6458c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5452d8471d643a9bcb6c2e96bf71ed0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aca116c1e0d547cc8380acc07e6c0a4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94433f4647e419788d8e9828a0d3a12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}